{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003db2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18456fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(symbol: str):\n",
    "    \"\"\"\n",
    "    Download stock data from Yahoo Finance from Jan 1, 2025 to today.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get today's date and the date two years ago\n",
    "        start_date = '2004-01-01'\n",
    "        end_date = '2024-12-31'\n",
    "\n",
    "\n",
    "        # Ensure the 'data' directory exists, create if not\n",
    "        directory = 'hist_data'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Download the stock data\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
    "        \n",
    "        # Save the downloaded data to a CSV file in the 'data' directory\n",
    "        file_path = os.path.join(directory, f'{symbol}.csv')\n",
    "        stock_data.to_csv(file_path)\n",
    "\n",
    "        print(f\"✅ Downloaded recent data saved to {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e145da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "def preprocess_data(file_path,ticker):\n",
    "    try:\n",
    "        # Load the raw data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Drop first two/three rows if metadata is detected (e.g., ticker name)\n",
    "        try:\n",
    "            pd.to_datetime(df.iloc[2, 0])  # Try to parse third row as date\n",
    "            df = df.iloc[2:].copy()\n",
    "        except Exception:\n",
    "            df = df.iloc[3:].copy()\n",
    "\n",
    "        # Rename 'Price' to 'Date' if necessary\n",
    "        if 'Price' in df.columns:\n",
    "            df.rename(columns={'Price': 'Date'}, inplace=True)\n",
    "\n",
    "        # Expected clean column list\n",
    "        expected_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        df = df[[col for col in expected_cols if col in df.columns]]\n",
    "\n",
    "        # Convert 'Date' to datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "        # Convert price-related columns to numeric\n",
    "        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Remove rows with missing or invalid values\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Save cleaned data\n",
    "        os.makedirs('historical_data', exist_ok=True)\n",
    "        clean_file_path = os.path.join('historical_data', f'{ticker}.csv')\n",
    "        df.to_csv(clean_file_path, index=False)\n",
    "\n",
    "        print(f\"✅ Cleaned data saved to: {clean_file_path}\")\n",
    "        return clean_file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in preprocessing: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb14c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'META'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d637139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded recent data saved to hist_data\\META.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = download_stock_data(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b369a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data saved to: historical_data\\META.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'historical_data\\\\META.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data(path,ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba357cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f244942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
